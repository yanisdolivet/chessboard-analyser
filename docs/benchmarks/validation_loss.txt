

#############################
# New run start at 20251220-183818
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-183818
# Model Specifications:
1.1136232614517212
1.112196683883667
1.1105940341949463
1.1083416938781738
1.105499029159546
1.1021208763122559
1.098644495010376
1.0952433347702026
1.0916887521743774
1.0880370140075684
1.084179401397705
1.0807476043701172
1.0773100852966309
1.073799967765808
1.0705386400222778
1.0674666166305542
1.0649266242980957
1.0625402927398682
1.0603704452514648
1.05852472782135
1.057153582572937
1.0560485124588013
1.0551949739456177
1.0548175573349
1.0547329187393188
1.0545457601547241
1.0544309616088867
1.0544116497039795
1.0545969009399414
1.0548362731933594
1.0553967952728271
1.0561537742614746
1.0571376085281372
1.0579984188079834
1.058971643447876
1.059957504272461
1.0608258247375488
1.0616755485534668
1.0626972913742065
1.0635344982147217
1.064211130142212
1.0647804737091064
1.0653114318847656
1.0658453702926636
1.0662261247634888
1.066548466682434
1.0667989253997803
1.06700599193573
1.0673176050186157
1.0675204992294312
1.067618727684021
1.0674681663513184
1.067415475845337
1.067334771156311
1.067274808883667
1.0670320987701416
1.066758394241333
1.0664901733398438
1.0666124820709229
1.0667911767959595
1.0670838356018066
1.0675156116485596
1.0678516626358032
1.0679965019226074
1.0679066181182861
1.0678367614746094
1.0676144361495972
1.0672475099563599
1.0669633150100708
1.0667970180511475
1.066861867904663
1.0669127702713013
1.0667564868927002
1.066657304763794
1.066704273223877
1.0669310092926025
1.0670444965362549
1.0670008659362793
1.066980242729187
1.0668606758117676
1.066718578338623
1.0666368007659912
1.0663728713989258
1.066185712814331
1.066052794456482
1.0657308101654053
1.0653841495513916
1.0650954246520996
1.0649504661560059
1.0647423267364502
1.0648499727249146
1.0647590160369873
1.064723014831543
1.064758539199829
1.064925193786621
1.065118432044983
1.0653260946273804
1.0656547546386719
1.0660502910614014
1.0664657354354858


#############################
# New run start at 20251220-192327
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-192327
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################
1.034782886505127
1.0335798263549805
1.0316964387893677
1.0296297073364258
1.0273149013519287
1.0249420404434204
1.0225050449371338
1.0199730396270752
1.0174765586853027
1.014937162399292
1.0122184753417969
1.0095419883728027
1.0073142051696777
1.0053365230560303
1.0036838054656982
1.0023677349090576
1.0012670755386353
1.0003032684326172
0.9996779561042786
0.9994522333145142
0.9993414282798767
0.9994280338287354
0.9997335076332092
1.0002734661102295
1.0008374452590942
1.0014137029647827
1.0020087957382202
1.0028960704803467
1.0038241147994995
1.0046314001083374
1.0054960250854492
1.0064841508865356
1.0073440074920654
1.0083128213882446
1.0096406936645508
1.0109739303588867
1.0122610330581665
1.0135704278945923
1.014763593673706
1.0158578157424927
1.0169413089752197
1.0178980827331543
1.0187493562698364
1.0194530487060547
1.0201855897903442
1.0209085941314697
1.0218214988708496
1.0225998163223267
1.0232365131378174
1.0236965417861938
1.0243101119995117
1.0251116752624512
1.0258519649505615
1.026534080505371
1.0270943641662598
1.0276832580566406
1.0281189680099487
1.0283823013305664
1.0286977291107178
1.0292636156082153
1.0300559997558594
1.0309075117111206
1.0318318605422974
1.0327765941619873
1.0338563919067383
1.03493332862854
1.0359420776367188
1.0370198488235474
1.0380208492279053
1.0389103889465332
1.0397653579711914
1.040590524673462
1.0414612293243408
1.0420655012130737
1.0427888631820679
1.0436105728149414
1.0443531274795532
1.045271396636963
1.046114444732666
1.046950101852417
1.047597050666809
1.0482865571975708
1.048903226852417
1.049437165260315
1.049917459487915
1.0503424406051636
1.0507116317749023
1.0510436296463013
1.0514311790466309
1.0516483783721924
1.0518136024475098
1.0520800352096558
1.0522890090942383
1.0525412559509277
1.0527011156082153
1.0528448820114136
1.0528608560562134
1.0529369115829468
1.0529953241348267
1.0532211065292358


#############################
# New run start at 20251220-192348
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-192421
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-192421
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################
0.9870110473632813
0.9487457885742188
0.9189439697265624
0.8958909301757813
0.8731027221679688
0.8527708740234375
0.82982373046875
0.8079546508789063
0.786824462890625
0.763018310546875
0.7420311279296875
0.7197349853515626
0.69880419921875
0.6777825317382813
0.6562001953125
0.64298876953125
0.6184247436523438
0.6017500610351563
0.584998046875
0.56844873046875
0.55556494140625
0.5423657836914062
0.5296936645507813
0.5176015625
0.5081603698730469
0.49594830322265626
0.4857554931640625
0.47660824584960937
0.46463311767578125
0.46126275634765623
0.44881204223632815
0.4409313659667969
0.43347601318359374
0.4260745849609375
0.4179432373046875
0.4098040771484375
0.40264312744140623
0.40173828125
0.3931153564453125
0.3896700439453125
0.38242010498046874
0.376925048828125
0.37260305786132814
0.3693785400390625
0.363830810546875
0.36033981323242187
0.35645672607421874
0.352261474609375
0.351490234375
0.3482406005859375
0.3432352294921875
0.3405216369628906
0.338669921875
0.334814453125
0.33053515625
0.33042312622070313
0.32737844848632813
0.3245506286621094
0.32008251953125
0.3184033813476562
0.31802203369140625
0.31382571411132815
0.3122759094238281
0.3112458190917969
0.3087754821777344
0.3089405517578125
0.30745208740234375
0.30568508911132813
0.30574310302734375
0.30284182739257814
0.30271197509765624
0.2990480041503906
0.29826708984375
0.2979705200195312
0.2958099060058594
0.2950445251464844
0.29482647705078124
0.29377838134765627
0.2896029052734375
0.2900487365722656
0.2880858459472656
0.29014306640625
0.2880645751953125
0.286165283203125
0.2863583068847656
0.28380792236328123
0.28365216064453125
0.2835141296386719
0.28206515502929685
0.28074169921875
0.2811160888671875
0.2800694580078125
0.27900592041015626
0.27696197509765624
0.2763888549804687
0.2763983764648438
0.2777239990234375
0.2757422485351563
0.27671697998046874
0.2736102600097656


#############################
# New run start at 20251220-194401
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-194701
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=linear
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-194701
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=linear
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################
0.984273681640625
0.9471775512695313
0.9214859008789062
0.899441162109375
0.8808743896484375
0.8619378662109375
0.8422465209960938
0.8248787231445313
0.8025936279296875
0.780183349609375
0.7610777587890625
0.7364511108398437
0.7146265869140624
0.69187646484375
0.6713135986328125
0.6490454711914062
0.6303248901367188
0.6091128540039062
0.59297216796875
0.5736052856445313
0.5583902587890625
0.5471187133789063
0.5327493896484375
0.518531982421875
0.5061320495605469
0.4949888610839844
0.48389276123046876
0.4752084350585937
0.46389691162109375
0.45565869140625
0.4466456604003906
0.4371576538085937
0.42731982421875
0.41867019653320314
0.41263629150390624
0.4058105163574219
0.4011607971191406
0.39073007202148435
0.38638375854492185
0.38059808349609375
0.37395892333984376
0.37063873291015625
0.366150146484375
0.35905792236328127
0.3551693115234375
0.3513650817871094
0.3472273254394531
0.3439825744628906
0.3389495849609375
0.33791143798828127
0.3323544921875
0.32924484252929687
0.3280374145507812
0.3233171081542969
0.32292010498046875
0.3183046875
0.316361328125
0.31249200439453123
0.31218609619140625
0.30707135009765624
0.30579425048828124
0.3020018005371094
0.3007359619140625
0.3008223876953125
0.296376953125
0.29461993408203124
0.2943574523925781
0.29129983520507813
0.28991189575195314
0.288454345703125
0.28638796997070315
0.2848290100097656
0.2832042236328125
0.282052490234375
0.2798695068359375
0.28056097412109376
0.27882659912109375
0.27722613525390627
0.2755820007324219
0.27417520141601565
0.27353329467773435
0.27161572265625
0.270338623046875
0.2704390869140625
0.26986630249023436
0.26868353271484374
0.2664698181152344
0.2672467346191406
0.2649514465332031
0.2661329345703125
0.2658299560546875
0.26347381591796876
0.2652061767578125
0.26425225830078125
0.26209222412109373
0.26021719360351564
0.26089077758789064
0.25877493286132813
0.25879888916015625
0.259878173828125
