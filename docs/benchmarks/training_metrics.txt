

#############################
# New run start at 20251220-183818
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################
0.6153846153846154
0.46153846153846156
0.5384615384615384
0.5384615384615384
0.6153846153846154
0.6153846153846154
0.5384615384615384
0.6153846153846154
0.6153846153846154
0.6153846153846154
0.5384615384615384
0.6923076923076923
0.6923076923076923
0.7692307692307693
0.7692307692307693
0.6923076923076923
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.6923076923076923
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.6923076923076923
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.6923076923076923
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.8461538461538461
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693


#############################
# New run start at 20251220-192327
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-192327
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################
0.38461538461538464
0.3076923076923077
0.38461538461538464
0.23076923076923078
0.46153846153846156
0.38461538461538464
0.38461538461538464
0.46153846153846156
0.46153846153846156
0.5384615384615384
0.6153846153846154
0.6923076923076923
0.6153846153846154
0.7692307692307693
0.7692307692307693
0.6153846153846154
0.7692307692307693
0.6923076923076923
0.6923076923076923
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.6923076923076923
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.6923076923076923
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.8461538461538461
0.8461538461538461
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.7692307692307693
0.7692307692307693
0.7692307692307693
0.8461538461538461
0.8461538461538461


#############################
# New run start at 20251220-192348
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-192421
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-192421
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################
0.45075
0.536375
0.561375
0.57525
0.58325
0.592
0.60175
0.6126875
0.627875
0.6395
0.64875
0.6638125
0.678125
0.692125
0.701875
0.708125
0.7170625
0.7280625
0.7360625
0.7423125
0.749375
0.7529375
0.7655625
0.772875
0.7776875
0.7824375
0.78975
0.792
0.794
0.801125
0.806125
0.8095
0.8125625
0.817
0.826375
0.82725
0.8293125
0.8284375
0.8405
0.8410625
0.84025
0.8483125
0.8485625
0.851125
0.8500625
0.855875
0.8559375
0.8631875
0.8603125
0.8631875
0.8645
0.8675
0.8734375
0.87325
0.873
0.876
0.88
0.8783125
0.8798125
0.88425
0.8865
0.8885625
0.8900625
0.891
0.8909375
0.89225
0.8953125
0.8935
0.8940625
0.8971875
0.9006875
0.8993125
0.9034375
0.903625
0.90025
0.9061875
0.9095625
0.908375
0.9099375
0.9121875
0.9106875
0.912875
0.9145
0.914
0.916375
0.9166875
0.9201875
0.917375
0.9183125
0.9175625
0.92325
0.919625
0.9205
0.921
0.923
0.924125
0.9254375
0.9276875
0.9251875
0.9264375


#############################
# New run start at 20251220-194401
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-194701
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=linear
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-194701
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=linear
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################
0.4549375
0.5330625
0.5665625
0.5758125
0.5846875
0.595375
0.6004375
0.6109375
0.6193125
0.630875
0.6413125
0.653125
0.665875
0.6796875
0.68475
0.699875
0.7080625
0.722
0.7274375
0.7304375
0.7415625
0.7490625
0.7540625
0.759875
0.7646875
0.7723125
0.7789375
0.7805625
0.788375
0.79375
0.793875
0.802125
0.8065
0.8089375
0.810375
0.8180625
0.8225625
0.82425
0.8316875
0.832625
0.8365625
0.837375
0.843
0.8416875
0.8474375
0.8476875
0.8505
0.8525
0.85375
0.8595
0.856875
0.8625
0.865125
0.8634375
0.866875
0.86875
0.870625
0.872875
0.8759375
0.8785625
0.8805
0.8813125
0.8823125
0.880625
0.88125
0.885875
0.8848125
0.8874375
0.888125
0.8914375
0.8905625
0.8945
0.8954375
0.896625
0.8979375
0.8986875
0.8974375
0.8994375
0.9029375
0.90625
0.904875
0.903125
0.9044375
0.907875
0.909625
0.9061875
0.91225
0.91575
0.912625
0.9133125
0.9183125
0.9125625
0.9169375
0.9193125
0.9220625
0.916375
0.9198125
0.9238125
0.919125
0.92425
