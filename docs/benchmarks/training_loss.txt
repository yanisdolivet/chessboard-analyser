

#############################
# New run start at 20251220-183818
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################
2.7095951614379885
2.7996626405165745
2.7158248960238236
2.7548316291478963
2.719623028058272
2.6561098648493107
2.677412550577751
2.6516990377536187
2.728010605115157
2.649978814821977
2.70375698023576
2.5664476604461672
2.6047393084305983
2.628408274944012
2.578365471684016
2.5993314269506014
2.497517397605456
2.5272105386623966
2.511088911496676
2.535162650135847
2.581476441997748
2.549775557921483
2.5412274077855623
2.4832341994689062
2.3917472413136407
2.48204094012884
2.4506736736847805
2.4047969880012365
2.458535038617941
2.45935958135128
2.3560323560146186
2.4041480758465252
2.3790352294628434
2.4463363579419943
2.367653136409246
2.314985289371931
2.33929121363163
2.448565689967229
2.3701971753193782
2.3549158727205715
2.345237201635654
2.31917434157775
2.2256931228821095
2.3190053795301
2.3439114001714265
2.3038109027055595
2.3041124742031096
2.4089188676522326
2.388281693100929
2.3727934572696685
2.2690982425029462
2.315364178162355
2.243841910398923
2.318104198749249
2.373330550633944
2.31096639419519
2.3124100415523237
2.271975328335395
2.140853999733925
2.2036341759608344
2.2252147815961103
2.3303554785068217
2.3657509053670442
2.28543365252935
2.187428055341427
2.1516259209284416
2.2920635708203685
2.2211595365267534
2.2444572935746265
2.093709208029967
2.047112016567817
2.1580766901511415
2.2996312733338433
2.291881977750705
2.1776032442404674
2.228932537812453
2.287399821941669
2.132874312520027
2.3218457015661094
2.1888593651606487
2.1528715379971723
2.197387780611332
2.2499500705462236
2.2939969311677495
2.1494362960045157
2.1706734270591004
2.147896069269914
2.1575344289632943
2.2154246080655318
2.2498150171775086
2.1741429256659286
2.1758861693418945
2.1512753746968047
2.3234544059404962
2.0749605400745685
2.098388520167424
2.1223280702554264
2.184088069292215
2.221626235769345
2.2385898104447586


#############################
# New run start at 20251220-192327
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-192327
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################
2.7330237416814303
2.7700545816655895
2.7267562870295157
2.778859269275266
2.7331637120933445
2.772999799191506
2.7229988367595235
2.698012142505709
2.6758952905610673
2.6618492532259475
2.6297322928283298
2.61296225833117
2.6516516065881883
2.577084564796208
2.534895480797638
2.6074618221788812
2.573849447432428
2.530619452483734
2.519234624383297
2.5374815920468374
2.537723861580571
2.5218680955954236
2.534263554146407
2.4720538166890944
2.4506340024411957
2.460705289724178
2.3687009945120296
2.337846203447941
2.498748803213087
2.3960082500260445
2.414708641107116
2.368631675050134
2.365408595835815
2.372317165217062
2.401582989259348
2.354302567781691
2.386638351922946
2.259283902650255
2.365319604713179
2.4540355192554797
2.3586755007081472
2.3021505277828247
2.2771840821437523
2.3084897038314147
2.2906515097357376
2.3125279647029533
2.2486644562945495
2.310072592698512
2.2601928937521
2.292756650134944
2.270418930272191
2.2697436489945915
2.365662188041329
2.310976391018885
2.2141313025547937
2.3988610174981178
2.40727606607848
2.2916955455581225
2.309363626987889
2.2052163931710913
2.3481061329773856
2.204163210843273
2.269917814833943
2.214845421168544
2.2523584255719378
2.23263339468157
2.1692218418209346
2.1326185068984183
2.1581513985382643
2.291572388839807
2.268174358787536
2.2136056479372535
2.2688566326422075
2.198870185536714
2.2082084477916215
2.186809663355466
2.2125990905437316
2.2879402941595126
2.1955216123541
2.1285837114069386
2.19841931147083
2.164091378195621
2.137415141327329
2.264181259385703
2.1289400454950487
2.249745130979462
2.1995377138210768
2.1141737567823444
2.1060839891516507
2.2873059680632255
2.192965934779226
2.255139646693107
2.230423575609692
2.2426489010194617
2.1691421877479184
2.173485022715347
2.171089289275908
2.0923321859057262
2.173543209456093
2.090060759979155


#############################
# New run start at 20251220-192348
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-192421
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-192421
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################
2.69303066160472
2.643337763612021
2.6054171482865933
2.577171859808558
2.5514054718623207
2.5290491989029324
2.505322279422915
2.480468972652579
2.460013313434562
2.4324508344746354
2.411694972503727
2.385693171817452
2.359813743486031
2.335414068698809
2.312016053301388
2.2899559413514092
2.2687019011935217
2.247739863826135
2.223648683170777
2.206359340996018
2.1886681855689787
2.1744860918942854
2.1573131052947643
2.137557020704135
2.124177494917408
2.1100561941777665
2.095156536833024
2.0848894584956876
2.071736489885722
2.0547905320747692
2.036311251963617
2.027535162271473
2.0142998092666557
2.002198090529019
1.988224507852658
1.9837358911857677
1.9687380964471284
1.9615581409662943
1.9489633239749724
1.9363234052729823
1.9352834055252695
1.9181881827093772
1.9123758010963108
1.9046294561262045
1.8963639631686873
1.8880775749134793
1.878165809937902
1.8670957511988027
1.864214785356891
1.8523540911820189
1.8471810629547927
1.8400230717764103
1.8267497817531764
1.8238216361245887
1.8176077738214285
1.8108214271694292
1.8024889577319418
1.7958259206908247
1.7894935944401777
1.7833088799890553
1.7756720652130604
1.770100628846386
1.7625894700325395
1.759390071307601
1.75352616561549
1.7464861383794168
1.7374929156826295
1.7374615828959097
1.731100253883131
1.72310691535106
1.7176464418807578
1.710642353483028
1.7071808575201959
1.6997348292038563
1.6969456272817658
1.6875546929778733
1.681075070792887
1.6813014343884434
1.6745019687081641
1.6698516349769599
1.6650552254557647
1.6579358102045771
1.6562317068983052
1.6519668930200464
1.6457273905766159
1.641015709628114
1.6338859481182029
1.6363342630981434
1.627932241739616
1.6288382945111353
1.618740332244094
1.618047574420946
1.613739692323129
1.6103934198481387
1.6066039191366923
1.6033668359550461
1.596163694377337
1.5900635066282887
1.589045299227659
1.585754933649026


#############################
# New run start at 20251220-194401
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=sigmoid
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-194701
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=linear
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################


#############################
# New run start at 20251220-194701
# Model Specifications:
# Layer 0: Type=INPUT, Size=2305, Activation=none
# Layer 1: Type=HIDDEN1, Size=768, Activation=relu
# Layer 2: Type=HIDDEN2, Size=896, Activation=relu
# Layer 3: Type=OUTPUT, Size=3, Activation=linear
# Hyperparameters:
# Learning Rate=0.004999999888241291, Initialization=he_mixed_xavier
# Training Parameters:
# Batch Size=64, Lreg=0.0010000000474974513, Dropout Rate=0.20000000298023224, Epochs=100, Loss Function=categorical_crossentropy
#############################
2.6992017381292057
2.6461080395930243
2.6106358517814585
2.584234525712206
2.561398606761646
2.537656469040242
2.5175856953015017
2.494492604838519
2.473092410091247
2.449379336145661
2.426253490675152
2.4029610300997684
2.380346060914845
2.3494356925610504
2.334406216724507
2.3103372453332387
2.287197838883434
2.2633672530515234
2.245766223159085
2.2286010451343112
2.208533771929125
2.1900546075342295
2.1765449000075643
2.163047645554519
2.1477064299137743
2.1315608667161916
2.115879208791939
2.103632633322406
2.091527647534218
2.0744332870370306
2.061920320416864
2.0479756408542795
2.0390572205558084
2.025753243909704
2.0155412492360156
2.002741285770658
1.989859060868033
1.9832011653519779
1.968945337888566
1.962161460166599
1.9461690699890406
1.9394808115794997
1.9312533766237503
1.9245791966872852
1.9124598662091152
1.9026814855935443
1.8980619749005165
1.890178553428727
1.883725119755362
1.8739636475510644
1.8681026016337212
1.85944843023037
1.8523512236871726
1.847468369357417
1.836796159601837
1.829650404610415
1.8268875068644697
1.8202185730065554
1.8058448737683022
1.7994136909040026
1.7938889594295695
1.7875601628409663
1.7827931895572475
1.7787719571969272
1.77225853497433
1.7646814504588217
1.7602197209201123
1.7517149350886128
1.7497443930358458
1.745054325738032
1.737530220445095
1.731700019710077
1.726607182015791
1.717935024368696
1.717035348057011
1.7088561301267076
1.7053065787837105
1.698275848919793
1.695343629417973
1.6842070267419227
1.682973248037215
1.6837286270731764
1.6765932149430767
1.66841855477188
1.665114230680401
1.6627457327303514
1.6569674036802005
1.649083079385089
1.6467214073709258
1.6422895778401554
1.6387480019308864
1.637142174870171
1.6300421672860648
1.6242716024413992
1.6182834837201197
1.6185869630571708
1.6141968662138706
1.608528069138212
1.608794574641921
1.5986355271476813
